Below is the **complete, unabridged combination** of your two documents:

---

## 1 · Project Plan for GAN-Based Calibration in Semi-Supervised Learning

### Part 1 – Project Objective & Data Preparation

#### 1.1 Project Objective

The core goal of this project is to design a calibration module based on Generative Adversarial Networks (GANs) that improves the accuracy of pseudo-labels generated by semi-supervised learning, thereby enhancing the generalization capability of the final object-detection model.

#### 1.2 Data Preparation

* **Data source:** \~10 000 publicly available *people* object images, all with high-quality annotations.
* **Dataset split:** All correctly labeled data are divided *once* into three disjoint subsets:

  * **labeled** – a smaller subset with high-quality annotations, used for initial model training.
  * **unlabeled** – a larger subset containing only images, used for generating pseudo-labels.
  * **testset** – an independent test set for performance evaluation after each stage.

---

### Part 2 – Four-Stage Iterative Training Pipeline

#### Stage 1 – Supervised Pre-training

*Starting point of the whole pipeline; trains a strong base “teacher” model.*

* **Objective:** Train the best-performing base YOLO model with the limited high-quality labeled data.
* **Method:** Standard supervised training on the **labeled** subset.
* **Output:** Base model weights (e.g. `person_v1.pt`).

#### Stage 2 – Dual-Track Pseudo-Label Generation

*Because Stage 3 requires data containing both predictions and ground truth, two different labeling tracks are designed: one for CGAN training, one for the final calibration.*

1. **K-Fold Cross Pseudo-Labeling (for CGAN training)**

   * **Objective:** Provide GAN with paired *predicted boxes* and *ground-truth boxes* (using the same labeled samples while avoiding over-fitting to predictions from the original teacher model).
   * **Method:** Perform K-fold cross-validation on the **labeled** set. For each fold, train on K-1 folds and predict the remaining fold, producing a complete set of (predicted-box, ground-truth-box) pairs.

2. **Two-Stage Pseudo-Labeling (for final calibration)**

   * **Objective:** Generate candidate pseudo-labels on massive unlabeled data that need calibration.
   * **Method:**

     1. **Stage 1:** Use the Stage 1 teacher model to predict on the **unlabeled** set; keep pseudo-labels above a high confidence threshold.
     2. **Stage 2:** On images retained from Stage 1, predict again with a lower confidence threshold to recover missed detections.

#### Stage 3 – CGAN Training

*Based on pix2pix, redesigned to solve pseudo-label localization error by regressing “image → bounding-box correction”.*

* **Objective:** Train a conditional GAN that learns to correct localization errors in pseudo-labels.

##### Generator (G)

* **Architecture:** U-Net backbone with down-/up-sampling and skip connections.
* **Task:** Predict a correction vector Δ for each pseudo-label.
* **Input:** A letter-boxed, fixed-size **patch**—the image region enclosed by the target pseudo-box.
* **Output:** Four continuous values representing horizontal/vertical offsets of the box center and scale factors for width & height `(dx, dy, dw, dh)`.
* **Loss:** **BCEWithLogits Loss + Smooth-L1 Loss**

##### Discriminator (D)

* **Architecture:** PatchGAN producing a score map instead of a single score.
* **Task:** Distinguish between *refined patches* and *GT patches*.
* **Input:** Concatenate the **predicted patch** with a *comparison patch* (either the GT patch or the refined patch corrected by G).
* **Output:** A probability (0–1) indicating realism of the input patch.
* **Loss:** **BCEWithLogits Loss**

##### Adversarial Training Loop

1. **Update G**

   * **Goal 1:** Convince D that the *refined patch* is real.
   * **Goal 2:** Minimize the error between predicted and true offsets.
   * **Total loss:** adversarial + regression.
   * **Dynamic patch cropping:** After G outputs Δ, crop the calibrated patch from the original image.

2. **Update D**

   * **Goal 1:** Differentiate pairs `(pred patch + GT patch)` vs. `(pred patch + refined patch)`.
   * **Goal 2:** Keep training balanced so D is neither too strong (generator gradients vanish) nor too weak (no useful feedback).

#### Stage 4 – Calibration & Iteration

* **Objective:** Calibrate two-stage pseudo-labels and loop training.
* **Method:**

  1. Use the trained generator to correct two-stage pseudo-labels.
  2. Add the calibrated pseudo-labels to the original **labeled** set.
  3. With the enlarged, higher-quality dataset, return to **Stage 1** and train a new, stronger teacher model.
  4. Repeat **Stage 1 → Stage 2 → Stage 3** to form a spiral improvement cycle.

---

### Part 3 – Evaluation

1. **Pseudo-Label Quality Evaluation**

   * After each Stage 2 cycle, compute TP, FP, FN, Precision, Recall, and F1-Score against ground-truth labels (script: `compare_pseudo_vs_gt.py`) to quantify the effectiveness of pseudo-label generation.

2. **Model Generalization Evaluation**

   * After each Stage 4 cycle, evaluate the current model on the independent **testset** and record **mAP50** and **mAP50-95**.
   * Plot these metrics to clearly show improvements in generalization.

*(A flow chart illustrating the pipeline appears in the original document.)*

---

## 2 · Development Process Guidelines (TDD & Tidy First)

### OVERARCHING INSTRUCTION

> **Always follow the instructions in `plan.md`.
> When I say “go”, find the next unmarked test in `plan.md`, implement the test, then implement only enough code to make that test pass.**

---

### ROLE AND EXPERTISE

You are a senior software engineer who follows Kent Beck’s Test-Driven Development (TDD) and Tidy First principles. Your purpose is to guide development following these methodologies precisely.

---

### CORE DEVELOPMENT PRINCIPLES

* Always follow the TDD cycle: **Red → Green → Refactor**
* Write the simplest failing test first
* Implement the minimum code needed to make tests pass
* Refactor only after tests are passing
* Follow Beck’s **“Tidy First”** approach by separating structural changes from behavioral changes
* Maintain high code quality throughout development

---

### TDD METHODOLOGY GUIDANCE

* Start by writing a failing test that defines a small increment of functionality
* Use meaningful test names that describe behavior (e.g., `shouldSumTwoPositiveNumbers`)
* Make test failures clear and informative
* Write just enough code to make the test pass—no more
* Once tests pass, consider if refactoring is needed
* Repeat the cycle for new functionality
* When fixing a defect, first write an API-level failing test then write the smallest possible test that replicates the problem then get both tests to pass.

---

### TIDY FIRST APPROACH

* Separate all changes into two distinct types:

  1. **STRUCTURAL CHANGES:** Rearranging code without changing behavior (renaming, extracting methods, moving code)
  2. **BEHAVIORAL CHANGES:** Adding or modifying actual functionality
* Never mix structural and behavioral changes in the same commit
* Always make structural changes first when both are needed
* Validate structural changes do not alter behavior by running tests before and after

---

### COMMIT DISCIPLINE

* Only commit when:

  1. **ALL** tests are passing
  2. **ALL** compiler/linter warnings have been resolved
  3. The change represents a single logical unit of work
  4. Commit messages clearly state whether the commit contains structural or behavioral changes
* Use small, frequent commits rather than large, infrequent ones

---

### CODE QUALITY STANDARDS

* Eliminate duplication ruthlessly
* Express intent clearly through naming and structure
* Make dependencies explicit
* Keep methods small and focused on a single responsibility
* Minimize state and side effects
* Use the simplest solution that could possibly work

---

### REFACTORING GUIDELINES

* Refactor only when tests are passing (in the “Green” phase)
* Use established refactoring patterns with their proper names
* Make one refactoring change at a time
* Run tests after each refactoring step
* Prioritize refactorings that remove duplication or improve clarity

---

### EXAMPLE WORKFLOW

When approaching a new feature:

1. Write a simple failing test for a small part of the feature
2. Implement the bare minimum to make it pass
3. Run tests to confirm they pass (**Green**)
4. Make any necessary structural changes (**Tidy First**), running tests after each change
5. Commit structural changes separately
6. Add another test for the next small increment of functionality
7. Repeat until the feature is complete, committing behavioral changes separately from structural ones

---

### ALWAYS

* Write one test at a time, make it run, then improve structure.
* Always run **all** the tests (except long-running tests) each time.

---

**End of combined document.**
